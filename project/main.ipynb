{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nalgorithms :\\n\\n    SAE training :\\n\\ndataset = base model dataset\\nactivations = truncated_model(dataset)\\nSAE fit activations\\n\\n    probe training :\\n\\ndataset = string + label\\nactivations = truncated_model(dataset)\\n    -> stop_at_layer (from transformer lens)\\noptional : activations = SAE(activations)\\nactivations = trivial_normalisation(activations)\\n\\ndataset = activations + label\\nloss = CCS or other\\nprobe = probe\\ntrain probe\\n\\ntest generalisation\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "algorithms :\n",
    "\n",
    "    SAE training :\n",
    "\n",
    "dataset = base model dataset\n",
    "activations = truncated_model(dataset)\n",
    "SAE fit activations\n",
    "\n",
    "    probe training :\n",
    "\n",
    "dataset = string + label\n",
    "activations = truncated_model(dataset)\n",
    "    -> stop_at_layer (from transformer lens)\n",
    "optional : activations = SAE(activations)\n",
    "activations = trivial_normalisation(activations)\n",
    "\n",
    "dataset = activations + label\n",
    "loss = CCS or other\n",
    "probe = probe\n",
    "train probe\n",
    "\n",
    "test generalisation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformer_lens as tl\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from probes import LinearProbe\n",
    "from losses import L_CCS\n",
    "from dataset_loader import test_generalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(model, dataset, stop_at_layer=None):\n",
    "    \"\"\"\n",
    "    Get activations from model on dataset.\n",
    "    Dataset should be a tensor.\n",
    "    \"\"\"\n",
    "    activations = model(dataset, stop_at_layer=stop_at_layer)\n",
    "\n",
    "    return activations\n",
    "\n",
    "def get_sparse_activations(activations, sae):\n",
    "    return sae(activations)\n",
    "\n",
    "def normalisation(data):\n",
    "    \"\"\"\n",
    "    Normalise data.\n",
    "    \"\"\"\n",
    "    \n",
    "    mu = torch.mean(data, dim=0, keepdim=True)\n",
    "    std = torch.std(data, dim=0, keepdim=True)\n",
    "    data = (data - mu) / std\n",
    "\n",
    "    return data\n",
    "\n",
    "def anisotropic_normalisation(data):\n",
    "    \"\"\"\n",
    "    Normalise anisotropic data.\n",
    "    \"\"\"\n",
    "\n",
    "    # activation shape : (n_points, n_features)\n",
    "    # mu shape : (1, n_features)\n",
    "    # cov shape : (n_features, n_features)\n",
    "\n",
    "    mu = torch.mean(data, dim=0, keepdim=True)\n",
    "    cov = torch.cov(data.T)\n",
    "    inv_cov = torch.inverse(cov)\n",
    "\n",
    "    \"\"\"\n",
    "    We want newCov=(N−1)^−1 A X X^T A^T = Id\n",
    "    So      (N-1)^-1 X X^T = A^-1 A^-T\n",
    "    So      inv_cov = A^T A\n",
    "    So      V L V^T = A^T A\n",
    "    So      A = (V L^1/2)^T\n",
    "    \"\"\"\n",
    "\n",
    "    L, V = torch.linalg.eig(inv_cov)\n",
    "    sqrt_L = torch.sqrt(L)\n",
    "    sqrt_inv_cov = torch.matmul(V, torch.matmul(torch.diag(sqrt_L), V.T)).real\n",
    "    sqrt_inv_cov = sqrt_inv_cov.T\n",
    "\n",
    "    data = torch.matmul(sqrt_inv_cov, (data - mu).T).T\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_probe(probe, train_loader, test_loader, loss_fn, n_epochs=10, lr=1e-3, verbose=False):\n",
    "    \"\"\"\n",
    "    Train probe on train_loader.\n",
    "    \"\"\"\n",
    "\n",
    "    optimizer = torch.optim.Adam(probe.parameters(), lr=lr)\n",
    "\n",
    "    iter = tqdm(range(n_epochs)) if verbose else range(n_epochs)\n",
    "    for epoch in iter:\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            pos, neg = data\n",
    "            pos_out = torch.sigmoid(probe(pos))\n",
    "            neg_out = torch.sigmoid(probe(neg))\n",
    "\n",
    "            loss = loss_fn(pos_out, neg_out)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(test_loader):\n",
    "                pos, neg = data\n",
    "                pos_out = torch.sigmoid(probe(pos))\n",
    "                neg_out = torch.sigmoid(probe(neg))\n",
    "\n",
    "                loss = loss_fn(pos_out, neg_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = #...\n",
    "dataset = #...\n",
    "\n",
    "sae_layer = #...\n",
    "sae = #...\n",
    "\n",
    "d_resid = model.cfg.d_model\n",
    "probe = LinearProbe(d_resid, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = get_activations(model, dataset, stop_at_layer=sae_layer)\n",
    "activations = sae(activations)\n",
    "activations = anisotropic_normalisation(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(activations, batch_size=32)\n",
    "\n",
    "train_loader, test_loader = loader.split(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_probe(probe, train_loader, test_loader, L_CCS, n_epochs=10, lr=1e-3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test generalisation\n",
    "\n",
    "datasets = #...#redo all normalisation on activations and sae\n",
    "\n",
    "test_generalisation(datasets, probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "gpt2small = tl.HookedTransformer.from_pretrained(\"gpt2\")\n",
    "d_resid = gpt2small.cfg.d_model\n",
    "probe = LinearProbe(d_resid, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt2',\n",
       " 'gpt2-medium',\n",
       " 'gpt2-large',\n",
       " 'gpt2-xl',\n",
       " 'distilgpt2',\n",
       " 'facebook/opt-125m',\n",
       " 'facebook/opt-1.3b',\n",
       " 'facebook/opt-2.7b',\n",
       " 'facebook/opt-6.7b',\n",
       " 'facebook/opt-13b',\n",
       " 'facebook/opt-30b',\n",
       " 'facebook/opt-66b',\n",
       " 'EleutherAI/gpt-neo-125M',\n",
       " 'EleutherAI/gpt-neo-1.3B',\n",
       " 'EleutherAI/gpt-neo-2.7B',\n",
       " 'EleutherAI/gpt-j-6B',\n",
       " 'EleutherAI/gpt-neox-20b',\n",
       " 'stanford-crfm/alias-gpt2-small-x21',\n",
       " 'stanford-crfm/battlestar-gpt2-small-x49',\n",
       " 'stanford-crfm/caprica-gpt2-small-x81',\n",
       " 'stanford-crfm/darkmatter-gpt2-small-x343',\n",
       " 'stanford-crfm/expanse-gpt2-small-x777',\n",
       " 'stanford-crfm/arwen-gpt2-medium-x21',\n",
       " 'stanford-crfm/beren-gpt2-medium-x49',\n",
       " 'stanford-crfm/celebrimbor-gpt2-medium-x81',\n",
       " 'stanford-crfm/durin-gpt2-medium-x343',\n",
       " 'stanford-crfm/eowyn-gpt2-medium-x777',\n",
       " 'EleutherAI/pythia-14m',\n",
       " 'EleutherAI/pythia-31m',\n",
       " 'EleutherAI/pythia-70m',\n",
       " 'EleutherAI/pythia-160m',\n",
       " 'EleutherAI/pythia-410m',\n",
       " 'EleutherAI/pythia-1b',\n",
       " 'EleutherAI/pythia-1.4b',\n",
       " 'EleutherAI/pythia-2.8b',\n",
       " 'EleutherAI/pythia-6.9b',\n",
       " 'EleutherAI/pythia-12b',\n",
       " 'EleutherAI/pythia-70m-deduped',\n",
       " 'EleutherAI/pythia-160m-deduped',\n",
       " 'EleutherAI/pythia-410m-deduped',\n",
       " 'EleutherAI/pythia-1b-deduped',\n",
       " 'EleutherAI/pythia-1.4b-deduped',\n",
       " 'EleutherAI/pythia-2.8b-deduped',\n",
       " 'EleutherAI/pythia-6.9b-deduped',\n",
       " 'EleutherAI/pythia-12b-deduped',\n",
       " 'EleutherAI/pythia-70m-v0',\n",
       " 'EleutherAI/pythia-160m-v0',\n",
       " 'EleutherAI/pythia-410m-v0',\n",
       " 'EleutherAI/pythia-1b-v0',\n",
       " 'EleutherAI/pythia-1.4b-v0',\n",
       " 'EleutherAI/pythia-2.8b-v0',\n",
       " 'EleutherAI/pythia-6.9b-v0',\n",
       " 'EleutherAI/pythia-12b-v0',\n",
       " 'EleutherAI/pythia-70m-deduped-v0',\n",
       " 'EleutherAI/pythia-160m-deduped-v0',\n",
       " 'EleutherAI/pythia-410m-deduped-v0',\n",
       " 'EleutherAI/pythia-1b-deduped-v0',\n",
       " 'EleutherAI/pythia-1.4b-deduped-v0',\n",
       " 'EleutherAI/pythia-2.8b-deduped-v0',\n",
       " 'EleutherAI/pythia-6.9b-deduped-v0',\n",
       " 'EleutherAI/pythia-12b-deduped-v0',\n",
       " 'EleutherAI/pythia-160m-seed1',\n",
       " 'EleutherAI/pythia-160m-seed2',\n",
       " 'EleutherAI/pythia-160m-seed3',\n",
       " 'NeelNanda/SoLU_1L_v9_old',\n",
       " 'NeelNanda/SoLU_2L_v10_old',\n",
       " 'NeelNanda/SoLU_4L_v11_old',\n",
       " 'NeelNanda/SoLU_6L_v13_old',\n",
       " 'NeelNanda/SoLU_8L_v21_old',\n",
       " 'NeelNanda/SoLU_10L_v22_old',\n",
       " 'NeelNanda/SoLU_12L_v23_old',\n",
       " 'NeelNanda/SoLU_1L512W_C4_Code',\n",
       " 'NeelNanda/SoLU_2L512W_C4_Code',\n",
       " 'NeelNanda/SoLU_3L512W_C4_Code',\n",
       " 'NeelNanda/SoLU_4L512W_C4_Code',\n",
       " 'NeelNanda/SoLU_6L768W_C4_Code',\n",
       " 'NeelNanda/SoLU_8L1024W_C4_Code',\n",
       " 'NeelNanda/SoLU_10L1280W_C4_Code',\n",
       " 'NeelNanda/SoLU_12L1536W_C4_Code',\n",
       " 'NeelNanda/GELU_1L512W_C4_Code',\n",
       " 'NeelNanda/GELU_2L512W_C4_Code',\n",
       " 'NeelNanda/GELU_3L512W_C4_Code',\n",
       " 'NeelNanda/GELU_4L512W_C4_Code',\n",
       " 'NeelNanda/Attn_Only_1L512W_C4_Code',\n",
       " 'NeelNanda/Attn_Only_2L512W_C4_Code',\n",
       " 'NeelNanda/Attn_Only_3L512W_C4_Code',\n",
       " 'NeelNanda/Attn_Only_4L512W_C4_Code',\n",
       " 'NeelNanda/Attn-Only-2L512W-Shortformer-6B-big-lr',\n",
       " 'NeelNanda/SoLU_1L512W_Wiki_Finetune',\n",
       " 'NeelNanda/SoLU_4L512W_Wiki_Finetune',\n",
       " 'ArthurConmy/redwood_attn_2l',\n",
       " 'llama-7b-hf',\n",
       " 'llama-13b-hf',\n",
       " 'llama-30b-hf',\n",
       " 'llama-65b-hf',\n",
       " 'Llama-2-7b-hf',\n",
       " 'Llama-2-7b-chat-hf',\n",
       " 'Llama-2-13b-hf',\n",
       " 'Llama-2-13b-chat-hf',\n",
       " 'Baidicoot/Othello-GPT-Transformer-Lens',\n",
       " 'bert-base-cased',\n",
       " 'roneneldan/TinyStories-1M',\n",
       " 'roneneldan/TinyStories-3M',\n",
       " 'roneneldan/TinyStories-8M',\n",
       " 'roneneldan/TinyStories-28M',\n",
       " 'roneneldan/TinyStories-33M',\n",
       " 'roneneldan/TinyStories-Instruct-1M',\n",
       " 'roneneldan/TinyStories-Instruct-3M',\n",
       " 'roneneldan/TinyStories-Instruct-8M',\n",
       " 'roneneldan/TinyStories-Instruct-28M',\n",
       " 'roneneldan/TinyStories-Instruct-33M',\n",
       " 'roneneldan/TinyStories-1Layer-21M',\n",
       " 'roneneldan/TinyStories-2Layers-33M',\n",
       " 'roneneldan/TinyStories-Instuct-1Layer-21M',\n",
       " 'roneneldan/TinyStories-Instruct-2Layers-33M',\n",
       " 'stabilityai/stablelm-base-alpha-3b',\n",
       " 'stabilityai/stablelm-base-alpha-7b',\n",
       " 'stabilityai/stablelm-tuned-alpha-3b',\n",
       " 'stabilityai/stablelm-tuned-alpha-7b',\n",
       " 'bigscience/bloom-560m',\n",
       " 'bigcode/santacoder']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.loading_from_pretrained.OFFICIAL_MODEL_NAMES"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
